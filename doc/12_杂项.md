在decoder-only的架构中，Tokenizer在训练的时候，padding side要选择right，在推理的时候，要选择left，因为推理的时候要预测下一个token，padding在右边无法获得有效的消息。



checkpoint就是有的激活值在反向传播的时候再重新计算，减少显存占用但是会增加训练时间



计算显存占用：

### **显存占用主要分五部分**

1.  **模型参数 (Parameters)**

    -   存储：`参数量 × 参数数据类型大小`
    -   例：float32 时，每参数占 **4 字节**。
    -   公式：`P × 4`

2.  **梯度 (Gradients)**

    -   每个参数对应一个梯度，大小与参数相同。
    -   公式：`P × 4`

3.  **优化器状态 (Optimizer States)**

    -   Adam 优化器为每个参数维护两个状态（一阶矩 `m` 和二阶矩 `v`）。
    -   状态通常用 float32 存储（即使模型用 float16）。
    -   公式：`P × 2 × 4 = P × 8`

4.  **前向激活值 (Activations)**

    -   与模型结构、序列长度 `seq_len`、批次大小 `batch_size` 相关。

    -   **近似公式**（Transformer Decoder）：

        激活值显存≈2×batch_size×seq_len×hidden_size×num_layers×(10+2×seq_len*hidden_size)

    -   **简化经验估计**（无梯度检查点时）：

        -   float32 激活：`~10 × P × 4`
        -   float16 激活：`~10 × P × 2`（混合精度训练）

5.  **输入数据 (Input Data)**

    -   输入 ID 和标签，通常为 int32 或 int64。
    -   公式：`2 × batch_size × seq_len × 4`（int32 时）



### **显存优化技巧**

1.  **混合精度训练**：
    使用 `torch.cuda.amp`，显存减少 40-50%。
2.  **梯度检查点**：
    通过 `torch.utils.checkpoint` 激活显存减少 5-10 倍（适合大模型）。
3.  **减小 `batch_size` 或 `seq_len`**：
    直接降低激活值显存。
4.  **分布式训练**：
    使用 DeepSpeed（ZeRO 阶段 2/3）或 FSDP 分摊优化器状态和梯度。





**Prompt Tuning** 是一种高效的**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）** 技术，其核心思想是**仅通过调整输入给预训练语言模型（PLM）的提示（Prompt）的可学习参数来适配下游任务，而保持预训练模型自身的参数完全冻结（不更新）**。

以下是其工作原理的详细分解：

1.  **核心概念：软提示（Soft Prompt / Prompt Embeddings）**
    -   不同于传统的“硬提示”（Hard Prompt，即人工设计或搜索得到的、由真实词汇组成的文本字符串，如 `"Translate English to French: {text}"`），Prompt Tuning 使用 **“软提示”**。
    -   软提示不是具体的单词，而是**一组连续的可学习向量（或称为“提示嵌入” - Prompt Embeddings）**。这些向量与模型的词嵌入（Word Embeddings）空间维度相同。
    -   可以将软提示想象成模型输入层之前的“虚拟令牌”（Virtual Tokens），它们没有直接的、人类可读的词汇对应，但模型能够理解和处理它们。
2.  **模型架构与输入构造**
    -   对于一个给定的输入序列 `X = [x1, x2, ..., xn]`（例如一个句子），首先将其转换为对应的词嵌入序列 `E(X) = [e1, e2, ..., en]`。
    -   创建一组可学习的提示嵌入 `P = [p1, p2, ..., pm]`，其中 `m` 是提示的长度（超参数）。
    -   将软提示 `P` **拼接**在输入序列的嵌入 `E(X)` **之前**，形成新的模型输入：`[P; E(X)] = [p1, p2, ..., pm, e1, e2, ..., en]`。
    -   这个拼接后的序列 `[P; E(X)]` 就是输入给**冻结的**预训练语言模型的完整输入。
3.  **前向传播（Forward Pass）**
    -   冻结的预训练模型像处理普通输入序列一样处理拼接后的序列 `[P; E(X)]`。
    -   模型根据其内部机制（通常是Transformer的自注意力机制）计算上下文表示。
    -   最终，模型会为序列中的特定位置（通常是原始输入 `X` 后面紧跟着的位置，或者一个特殊的 `[MASK]` 位置）输出预测结果（例如，预测下一个词或分类标签对应的词）。
4.  **损失计算与反向传播**
    -   根据任务定义损失函数（例如，分类任务的交叉熵损失，生成任务的负对数似然损失）。
    -   计算模型预测与真实标签之间的损失。
    -   **关键点：** 在反向传播过程中，**仅计算软提示嵌入 `P` 的梯度**。预训练模型的所有参数都被设置为不可训练（`requires_grad=False`），因此它们的梯度为零，不会被更新。
5.  **参数更新**
    -   使用优化器（如Adam）根据计算出的梯度**只更新软提示嵌入 `P` 中的参数**。
    -   预训练模型的参数保持不变。
6.  **迭代优化**
    -   重复步骤2-5，在训练数据上迭代多个epoch。
    -   通过优化，软提示嵌入 `P` 逐渐学习到如何引导冻结的预训练模型更好地执行特定的下游任务。`P` 本质上学习到了特定任务所需的知识激活模式或上下文信息。

代码如下：

```python
class PromptTuningModel(nn.Module):
    def __init__(self, base_model, hidden_size, prompt_length):
        super().__init__()
        self.base_model = base_model
        self.prompt_embeddings = nn.Embedding(prompt_length, hidden_size)
        self.prompt_length = prompt_length
        
        # 初始化提示嵌入 (使用预训练模型的输入嵌入进行初始化)
        pretrained_embeddings = base_model.get_input_embeddings().weight
        self.prompt_embeddings.weight.data = pretrained_embeddings[:prompt_length].clone().detach()
        
        # 创建虚拟令牌ID (仅用于前向传播)
        self.prompt_token_ids = torch.arange(prompt_length).long()
    
    def forward(self, input_ids, attention_mask, labels=None):
        # 获取输入词嵌入
        inputs_embeds = self.base_model.get_input_embeddings()(input_ids)
        
        # 获取提示嵌入 [batch_size, prompt_length, hidden_size]
        batch_size = input_ids.size(0)
        prompt_embeds = self.prompt_embeddings(
            self.prompt_token_ids.unsqueeze(0).expand(batch_size, -1).to(input_ids.device)
        
        # 拼接提示和输入 [batch_size, prompt_len + seq_len, hidden_size]
        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)
        
        # 扩展注意力掩码以包含提示
        prompt_attention_mask = torch.ones(batch_size, self.prompt_length).to(attention_mask.device)
        attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)
        
        # 通过冻结的基础模型
        outputs = self.base_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs
```



