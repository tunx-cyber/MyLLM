# 引言

GPT2是最适合LLM新手入门的模型。

推荐阅读材料：《从零开始构建大型语言模型》作者：Sebastian



# 数据集制作

在预训练阶段，LLMs逐词处理文本。使用数百万到数十亿参数的LLMs通过下一个词预测任务进行训练，产生了具有令人印象深刻能力的模型。然后，这些模型可以进一步微调以遵循一般指令或执行特定目标任务。但在我们可以实现和训练LLMs之前，我们需要准备训练数据集。

## 下载数据集

我们采用minimind清洗过的数据集，方便我们能够搭建真正能够对话的模型。

```shell
sudo apt-get update
sudo apt-get install git
sudo apt-get intall git-lfs
git lfs install
git clone https://www.modelscope.cn/datasets/gongjy/minimind_dataset.git
```

## 词嵌入

将数据转换为向量格式的概念通常称为嵌入。本质上，嵌入是从离散对象（如词语、图像或整篇文档）到连续向量空间中的点的映射——嵌入的主要目的是将非数值数据转换为神经网络可以处理的格式。

## 分词

分词是把输入文本拆分为单独的标记。

一种简单的方法就是把每个单词，标点符号，特殊符号等单独分出来。

然后，我们要将这些单独的标记转为标点ID。比如"a"映射为整数0

当然我们一般会加一些特殊的标记

```txt
[BOS]（序列开始）——此标记指示文本的开始。它告诉 LLM 内容从何处开始。
[EOS]（序列结束）——此标记位于文本末尾，当连接多个不相关的文本时特别有用，类似于 。例如，在组合两篇不同的维基百科文章或书籍时，[EOS] 标记指示一个文本的结束和下一个文本的开始。
[PAD]（填充）——当使用大于一的批量大小训练 LLM 时，批量中可能包含长度各异的文本。为了确保所有文本具有相同的长度，较短的文本使用 [PAD] 标记扩展或“填充”，直到与批量中最长的文本等长。
```

### 数据集中的mask

attention mask：经过tokenizer后，有最大长度和padding的设置，队友padding的位置，attention mask就设置为0，其余为1

loss mask是针对label的部分进行屏蔽，避免模型学习到这些连贯的语言。例如在指令微调中，对于prompt进行屏蔽，在label中对于这部分设置为-100

后续会详细讲解。

## BPE

### 核心思想：

BPE 是一种**数据驱动的子词分词算法**，其目标是通过不断**合并频率最高的字符对**来构造一个词汇表，从而减小词表规模，同时保留对罕见词的表达能力。

这类的思想和哈夫曼编码可以结合起来一起理解。

### 示例语料库

```python
corpus = [
    "low",
    "lowest",
    "newer",
    "wider"
]
```

### 准备语料

```python
from collections import defaultdict, Counter

# 初始化语料，每个词都加上结尾符号 </w>
def get_vocab(corpus):
    vocab = {}
    for word in corpus:
        chars = list(word) + ['</w>']
        word_str = ' '.join(chars)
        vocab[word_str] = vocab.get(word_str, 0) + 1
    return vocab

corpus = ["low", "lowest", "newer", "wider"]
vocab = get_vocab(corpus)
#{'l o w </w>': 1, 'l o w e s t </w>': 1, 'n e w e r </w>': 1, 'w i d e r </w>': 1}
```

### 计算所有 **相邻符号对（bigram）** 的频率

```python
def get_stats(vocab):
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pair = (symbols[i], symbols[i+1])
            pairs[pair] += freq
    return pairs
```

### 合并频率最高的 pair

```python
def merge_vocab(pair, vocab):
    merged_vocab = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)
    for word in vocab:
        new_word = word.replace(bigram, replacement)
        merged_vocab[new_word] = vocab[word]
    return merged_vocab
```

### 执行 N 次合并（即训练 BPE）

```python
num_merges = 10  # 控制词表大小
for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    print(f"Step {i+1}: merging {best}")
    vocab = merge_vocab(best, vocab)
```

### 最终词表（BPE tokens）

```python
def extract_tokens(vocab):
    tokens = set()
    for word in vocab:
        tokens.update(word.split())
    return tokens

tokens = extract_tokens(vocab)
print("BPE Vocabulary:", tokens)
```



### 使用第三方库

minimind里面使用tokenizer库进行训练，代码如下：

```python
def train_tokenizer():
    # 读取JSONL文件并提取文本数据
    def read_texts_from_jsonl(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                yield data['text']

    data_path = '../dataset/pretrain_hq.jsonl'

    # 初始化tokenizer
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

    # 定义特殊token
    special_tokens = ["<|endoftext|>", "<|im_start|>", "<|im_end|>"]

    # 设置训练器并添加特殊token
    trainer = trainers.BpeTrainer(
        vocab_size=6400,
        special_tokens=special_tokens,  # 确保这三个token被包含
        show_progress=True,
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
    )

    # 读取文本数据
    texts = read_texts_from_jsonl(data_path)

    # 训练tokenizer
    tokenizer.train_from_iterator(texts, trainer=trainer)

    # 设置解码器
    tokenizer.decoder = decoders.ByteLevel()

    # 检查特殊token的索引
    assert tokenizer.token_to_id("<|endoftext|>") == 0
    assert tokenizer.token_to_id("<|im_start|>") == 1
    assert tokenizer.token_to_id("<|im_end|>") == 2

    # 保存tokenizer
    tokenizer_dir = "../model/"
    os.makedirs(tokenizer_dir, exist_ok=True)
    tokenizer.save(os.path.join(tokenizer_dir, "tokenizer.json"))
    tokenizer.model.save("../model/")

    # 手动创建配置文件
    config = {
        "add_bos_token": False,
        "add_eos_token": False,
        "add_prefix_space": False,
        "added_tokens_decoder": {
            "0": {
                "content": "<|endoftext|>",
                "lstrip": False,
                "normalized": False,
                "rstrip": False,
                "single_word": False,
                "special": True
            },
            "1": {
                "content": "<|im_start|>",
                "lstrip": False,
                "normalized": False,
                "rstrip": False,
                "single_word": False,
                "special": True
            },
            "2": {
                "content": "<|im_end|>",
                "lstrip": False,
                "normalized": False,
                "rstrip": False,
                "single_word": False,
                "special": True
            }
        },
        "additional_special_tokens": [],
        "bos_token": "<|im_start|>",
        "clean_up_tokenization_spaces": False,
        "eos_token": "<|im_end|>",
        "legacy": True,
        "model_max_length": 32768,
        "pad_token": "<|endoftext|>",
        "sp_model_kwargs": {},
        "spaces_between_special_tokens": False,
        "tokenizer_class": "PreTrainedTokenizerFast",
        "unk_token": "<|endoftext|>",
        "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}"
    }

    # 保存配置文件
    with open(os.path.join(tokenizer_dir, "tokenizer_config.json"), "w", encoding="utf-8") as config_file:
        json.dump(config, config_file, ensure_ascii=False, indent=4)

    print("Tokenizer training completed and saved.")

```



## 滑动窗口

LLM在训练期间的任务是预测紧跟在输入块后面的下一个词。在训练过程中，我们屏蔽掉所有超过目标位置的词。

在没有格式化的数据情况下，我们使用一下方法

```python
class MyDataset(Dataset):
    def __init__(self, str_data, max_length=1024,stride=1,tokenizer = tiktoken.get_encoding("gpt2")):
        self.input_ids = []
        self.target_ids = []
        for item in str_data:
            token_ids = tokenizer.encode(item)
            for i in range(0,len(token_ids)-max_length, stride):
                input_chunk = token_ids[i : i+max_length]
                target_chunk = token_ids[i+1 : i+max_length+1]
                self.input_ids.append(torch.tensor(input_chunk))
                self.target_ids.append(torch.tensor(target_chunk))


    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]
```



如果有格式化的数据，我们可以采用minimind的方法

```python
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)

    def load_data(self, path):
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                data = json.loads(line.strip())
                samples.append(data)
        return samples

    def __len__(self):
        return len(self.samples)

    #这里一个样本的数据只使用了一次，并且超过最大长度的不会被使用到。可以改进。
    def __getitem__(self, index):
        sample = self.samples[index]

        # 构建输入文本
        encoding = self.tokenizer(
            str(sample['text']),
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        input_ids = encoding.input_ids.squeeze()
        loss_mask = (input_ids != self.tokenizer.pad_token_id)

        X = torch.tensor(input_ids[:-1], dtype=torch.long)
        Y = torch.tensor(input_ids[1:], dtype=torch.long)
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)
        return X, Y, loss_mask
```



# 创建词嵌入

这里就是前面分词得到的整数型的token转为嵌入向量。

```python
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
```

本质和hash表非常类似。嵌入层本质上是一个查找操作，通过词ID从嵌入层的权重矩阵中检索行。

比如token为0就找第0个向量，token为50就找第50个向量。

直观演示

```python
torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)

input_ids = torch.tensor([2, 3, 5, 1])
print(embedding_layer(input_ids))

Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)
tensor([[ 1.2753, -0.2010, -0.1606], #上面第2行 (从0行开始)
        [-0.4015,  0.9666, -1.1481],
        [-2.8400, -0.7849, -1.4096],
        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)
```



# 位置编码

LLM的一个小缺点是其自注意力机制没有对序列中词的位置或顺序的概念。

嵌入层将词ID转换为相同的向量表示，而不考虑它在输入序列中的位置。例如，无论词ID 5是在输入向量的第一个还是第四个位置，都会产生相同的嵌入向量。原则上，这种确定性的、与位置无关的词ID嵌入对于可重复性是有好处的。然而，由于LLM的自注意力机制本身也是位置无关的，因此将额外的位置信息注入LLM是有帮助的。

为此，我们可以使用两种广泛的位置感知嵌入：相对位置嵌入和绝对位置嵌入。绝对位置嵌入直接与序列中的特定位置相关联。对于输入序列中的每个位置，添加一个唯一的嵌入以传达其确切位置。

目前主流为RoPE，GPT2中使用的是绝对位置。

# Attention

注意力机制的核心思想是通过计算查询（query）和键（key）之间的相似度来决定模型在处理输入时应该关注哪些部分。具体来说，注意力机制通过计算查询和键之间的点积来衡量它们之间的相似度。

应用因果注意力掩码在自回归模型中，我们需要确保每个位置只能看到它之前的位置，以保持因果关系。 在实现中，我们可以使用一个上三角矩阵来实现这个掩码。具体来说，我们可以创建一个大小为(seq_len, seq_len)的上三角矩阵，并将其与注意力权重矩阵相乘。这样，只有上三角矩阵中对应位置为1的元素会被保留，而其他位置会被置为0。在实现中，我们可以使用torch.triu函数来创建上三角矩阵，并将其与注意力权重矩阵相乘。

**具体过程如下：**

整个过程为一个token经过$W_q$矩阵后变为一个向量q，形状为($d_q,1$)，经过$W_k$矩阵后变为一个向量k,形状为($d_k,1$)，经过$W_v$矩阵后变为一个向量v,形状为($d_v,1$)。

一个token的q分别和其他token的k做点乘，也就是相似度的衡量，得到这个token对其他token的注意力分数，然后每一个分数乘以每一个token的v向量，得到最终的输出。

其中为了一个token不能看到后面的，我们加了掩码，屏蔽了后面的token。

多头就是多个上述的步骤。

```python
class MultiHeadCausalAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len=512,drop_rate = 0.1,qkv_bias=False):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.d_model = d_model  # 模型维度
        self.n_heads = n_heads  # 注意力头数
        self.d_k = d_model // n_heads  # 每个头的维度

        # 线性投影层（Q/K/V/O）
        self.w_q = nn.Linear(d_model, d_model,bias=qkv_bias)  # 查询投影
        self.w_k = nn.Linear(d_model, d_model,bias=qkv_bias)  # 键投影
        self.w_v = nn.Linear(d_model, d_model,bias=qkv_bias)  # 值投影
        self.w_o = nn.Linear(d_model, d_model)  # 输出投影
        self.register_buffer("mask", torch.triu(
            torch.ones(max_len, max_len), diagonal=1)
        )  # 注册掩码缓冲区
        self.dropout = nn.Dropout(drop_rate)  # Dropout层

    def forward(self, x):
        """
        输入:
            x: 输入张量 (batch_size, seq_len, d_model)
            mask: 因果掩码 (batch_size, 1, seq_len, seq_len)
        输出:
            注意力后的张量 (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.shape #d_model和_相等

        # 1. 线性投影并分割多头 把_也就是d_model分为head*d_k
        q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (B, H, L, d_k)
        k = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (B, H, L, d_k)
        v = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (B, H, L, d_k)

        # 2. 计算缩放点积注意力
        attn_scores = torch.matmul(q, k.transpose(-2, -1))# (B, H, L, d_k) * (B, H, d_k, L) = (B, H, L, L)
        mask_bool = self.mask.bool()[:seq_len, :seq_len]
        # 3. 应用因果掩码（防止关注未来位置）
        attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)  # (B, H, L, L)

        attn_weights = F.softmax(attn_scores/(k.shape[-1]**0.5), dim=-1)  # (B, H, L, L)
        attn_weights = self.dropout(attn_weights)

        attn_output = torch.matmul(attn_weights, v)    #(B, H, L, L) * (B, H, L, d_k) = (B, H, L, d_k)

        # 4. 合并多头并输出投影
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (B, L, d_model)
        return self.w_o(attn_output)
```



# Transformer

完整的部分读者自行寻找，核心的部件就是如下：

```python
x = norm(x)
x = x + attention(x)

x = norm(x)
x = x + fnn(x)
```

GPT2中：

```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super(FeedForward, self).__init__()  
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            nn.GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )
    
    def forward(self,x):
        x = self.layers(x)
        return x
    
class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadCausalAttention(
            cfg["emb_dim"], 
            n_heads=cfg["n_heads"], 
            max_len=cfg["max_length"],
            drop_rate=cfg["drop_rate"], 
            qkv_bias=cfg["qkv_bias"])
        self.feed_forward = FeedForward(cfg)
        self.norm1 = nn.LayerNorm(cfg["emb_dim"])
        self.norm2 = nn.LayerNorm(cfg["emb_dim"])
        self.dropout = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        #prelayer norm
        shortcut = x
        x = self.norm1(x)
        x = self.attention(x)
        x = self.dropout(x)
        x = x + shortcut

        shortcut = x
        x = self.norm2(x)
        x = self.feed_forward(x)
        x = self.dropout(x)
        x = x + shortcut
        return x
```



# GPT2结构

```python
class MyGPT(nn.Module):
    def __init__(self,cfg):
        super(MyGPT, self).__init__()
        self.token_embedding = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.position_embedding = nn.Embedding(cfg["max_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])]
        )
        self.final_norm = nn.LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"],bias=False)

    def forward(self,in_idx):
        device = in_idx.device
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.token_embedding(in_idx)
        pos_embeds = self.position_embedding(torch.arange(seq_len, device=device))
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)


        for layer_idx, trf_block in enumerate(self.trf_blocks):
            x = trf_block(x)
        # x = self.trf_blocks(x,attn_mask)
        x = self.final_norm(x) # (B, L, d_model)
        logits = self.out_head(x) # (B, L, vocab_size)
        return logits
```

# 训练

```python
ef train_model(model, dataloader, optimizer,scheduler, criterion, logger, num_epochs=1):
    """
    训练模型
    :param model: 模型
    :param dataloader: 数据加载器
    :param optimizer: 优化器
    :param criterion: 损失函数
    :param num_epochs: 训练轮数
    """
    model.train()
    for epoch in range(num_epochs):
        for batch_idx, data in enumerate(dataloader):
            input_ids = data["input_ids"].to(device)
            target_ids = data["labels"].to(device)
            optimizer.zero_grad()
            outputs = model(input_ids)
            loss = criterion(
                outputs.flatten(0, 1),# [batch,seq_len,vocab_size(logits)]
                target_ids.flatten() #[batch_size, seq_len(id)]
            )
            loss.backward()
            optimizer.step()
            scheduler.step()
            if batch_idx % 10 == 0:
                logger.info(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}], Loss: {loss.item():.4f}')
        if epoch % 2 == 0:
            torch.save(model.state_dict(), f"checkpoints/model_epoch_{epoch}.pth")
```

# 生成文本

```python
import torch

def generate_text_simple(model, idx, max_new_tokens, context_size):
    for _ in range(max_new_tokens):
        # Crop the context to fit the model’s maximum context size
        idx_cond = idx if idx.size(1) <= context_size else idx[:,-context_size:]
        # Compute predictions
        logits = model(idx_cond)
        # Select the next token based on the highest probability prediction
        probs = torch.softmax(logits[:,-1, :], dim=-1)
        next_idx = torch.argmax(probs, dim=-1).unsqueeze(1)
        # Append the next token to the input sequence
        idx = torch.cat((idx, next_idx), dim=1)
    return idx

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)
    return encoded_tensor # 添加批次维度
def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0) # 移除批次维度
    return tokenizer.decode(flat.tolist())
```

目前除了这种外，还有加入一个温度系数，然后进行sample top k那种。

